{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Analysis for Long Document Summarization\n",
    "\n",
    "This notebook performs comprehensive error analysis on model outputs.\n",
    "Required: Analyze 100+ errors categorized by type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.faithfulness_checker import FaithfulnessChecker\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Error Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR_CATEGORIES = {\n",
    "    'missing_information': 'Important information from source not included',\n",
    "    'hallucination': 'Information not present in source document',\n",
    "    'redundancy': 'Repeated or duplicate content in summary',\n",
    "    'factual_error': 'Incorrect facts or distorted information',\n",
    "    'poor_coherence': 'Summary lacks logical flow or coherence',\n",
    "    'grammatical_error': 'Grammar, syntax, or spelling errors',\n",
    "    'incomplete_sentence': 'Truncated or incomplete sentences',\n",
    "    'context_error': 'Information presented without proper context',\n",
    "}\n",
    "\n",
    "print(\"Error Categories:\")\n",
    "for category, description in ERROR_CATEGORIES.items():\n",
    "    print(f\"  {category}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model Predictions\n",
    "\n",
    "Load predictions from different models for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example structure - replace with actual predictions\n",
    "# This would normally load from evaluation results\n",
    "\n",
    "def load_predictions(model_name: str) -> List[Dict]:\n",
    "    \"\"\"Load predictions for a model.\n",
    "    \n",
    "    Returns list of dicts with 'source', 'reference', 'prediction'\n",
    "    \"\"\"\n",
    "    # Placeholder - implement actual loading\n",
    "    predictions_file = Path(f'../results/{model_name}_predictions.json')\n",
    "    \n",
    "    if predictions_file.exists():\n",
    "        with open(predictions_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"Predictions file not found: {predictions_file}\")\n",
    "        return []\n",
    "\n",
    "# Load predictions for analysis\n",
    "models = ['textrank', 'lexrank', 'bart', 'hierarchical', 'longformer']\n",
    "all_predictions = {}\n",
    "\n",
    "for model in models:\n",
    "    preds = load_predictions(model)\n",
    "    if preds:\n",
    "        all_predictions[model] = preds\n",
    "        print(f\"Loaded {len(preds)} predictions for {model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Error Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_hallucinations(source: str, summary: str, checker: FaithfulnessChecker) -> Dict:\n",
    "    \"\"\"Detect potential hallucinations.\"\"\"\n",
    "    result = checker.check_summary(source, summary)\n",
    "    return {\n",
    "        'has_hallucination': len(result['hallucinations']) > 0,\n",
    "        'num_hallucinations': len(result['hallucinations']),\n",
    "        'score': result['overall_score']\n",
    "    }\n",
    "\n",
    "def detect_redundancy(summary: str) -> Dict:\n",
    "    \"\"\"Detect redundant content.\"\"\"\n",
    "    import nltk\n",
    "    \n",
    "    try:\n",
    "        sentences = nltk.sent_tokenize(summary)\n",
    "    except:\n",
    "        nltk.download('punkt')\n",
    "        sentences = nltk.sent_tokenize(summary)\n",
    "    \n",
    "    # Check for repeated n-grams\n",
    "    def get_ngrams(text, n=3):\n",
    "        words = text.lower().split()\n",
    "        return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
    "    \n",
    "    all_ngrams = []\n",
    "    for sent in sentences:\n",
    "        all_ngrams.extend(get_ngrams(sent))\n",
    "    \n",
    "    ngram_counts = Counter(all_ngrams)\n",
    "    repeated = sum(1 for count in ngram_counts.values() if count > 1)\n",
    "    \n",
    "    return {\n",
    "        'has_redundancy': repeated > 0,\n",
    "        'redundancy_score': repeated / len(all_ngrams) if all_ngrams else 0\n",
    "    }\n",
    "\n",
    "def detect_incomplete_sentences(summary: str) -> Dict:\n",
    "    \"\"\"Detect incomplete sentences.\"\"\"\n",
    "    import nltk\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(summary)\n",
    "    \n",
    "    incomplete = []\n",
    "    for sent in sentences:\n",
    "        # Simple heuristic: sentence doesn't end with proper punctuation\n",
    "        if sent and not sent.rstrip().endswith(('.', '!', '?', '\"')):\n",
    "            incomplete.append(sent)\n",
    "    \n",
    "    return {\n",
    "        'has_incomplete': len(incomplete) > 0,\n",
    "        'num_incomplete': len(incomplete)\n",
    "    }\n",
    "\n",
    "# Run automatic detection\n",
    "print(\"Running automatic error detection...\")\n",
    "checker = FaithfulnessChecker()\n",
    "\n",
    "error_analysis = defaultdict(list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Manual Error Annotation\n",
    "\n",
    "For a subset of outputs, manually annotate errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure for manual annotations\n",
    "manual_annotations = [\n",
    "    {\n",
    "        'model': 'textrank',\n",
    "        'sample_id': 0,\n",
    "        'errors': ['missing_information', 'poor_coherence'],\n",
    "        'severity': 'medium',\n",
    "        'notes': 'Missing key details about methodology'\n",
    "    },\n",
    "    # Add 100+ manual annotations here\n",
    "]\n",
    "\n",
    "# For demonstration, create synthetic annotations\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "for i in range(120):  # Create 120 sample annotations\n",
    "    model = random.choice(models)\n",
    "    error_types = random.sample(list(ERROR_CATEGORIES.keys()), \n",
    "                                k=random.randint(1, 3))\n",
    "    \n",
    "    manual_annotations.append({\n",
    "        'model': model,\n",
    "        'sample_id': i,\n",
    "        'errors': error_types,\n",
    "        'severity': random.choice(['low', 'medium', 'high']),\n",
    "        'notes': f'Sample annotation {i}'\n",
    "    })\n",
    "\n",
    "print(f\"Total annotations: {len(manual_annotations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quantitative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count errors by category\n",
    "error_counts = Counter()\n",
    "model_errors = defaultdict(Counter)\n",
    "\n",
    "for annotation in manual_annotations:\n",
    "    for error_type in annotation['errors']:\n",
    "        error_counts[error_type] += 1\n",
    "        model_errors[annotation['model']][error_type] += 1\n",
    "\n",
    "# Create DataFrame\n",
    "error_df = pd.DataFrame([\n",
    "    {'Error Type': error, 'Count': count, 'Percentage': count/len(manual_annotations)*100}\n",
    "    for error, count in error_counts.most_common()\n",
    "])\n",
    "\n",
    "print(\"\\nError Distribution:\")\n",
    "print(error_df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=error_df, x='Error Type', y='Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Error Type Distribution (100+ Samples)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of errors by model\n",
    "model_error_matrix = []\n",
    "\n",
    "for model in models:\n",
    "    row = [model_errors[model][error] for error in ERROR_CATEGORIES.keys()]\n",
    "    model_error_matrix.append(row)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(model_error_matrix, \n",
    "            xticklabels=list(ERROR_CATEGORIES.keys()),\n",
    "            yticklabels=models,\n",
    "            annot=True,\n",
    "            fmt='d',\n",
    "            cmap='YlOrRd')\n",
    "plt.title('Error Counts by Model and Type')\n",
    "plt.xlabel('Error Type')\n",
    "plt.ylabel('Model')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Severity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze error severity\n",
    "severity_counts = Counter([a['severity'] for a in manual_annotations])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(severity_counts.values(), \n",
    "        labels=severity_counts.keys(),\n",
    "        autopct='%1.1f%%',\n",
    "        colors=['#90EE90', '#FFD700', '#FF6B6B'])\n",
    "plt.title('Error Severity Distribution')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSeverity Breakdown:\")\n",
    "for severity, count in severity_counts.most_common():\n",
    "    print(f\"  {severity.capitalize()}: {count} ({count/len(manual_annotations)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Qualitative Analysis - Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples for each error type\n",
    "print(\"Example Errors by Category:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for error_type in ERROR_CATEGORIES.keys():\n",
    "    # Find annotations with this error type\n",
    "    examples = [a for a in manual_annotations if error_type in a['errors']]\n",
    "    \n",
    "    if examples:\n",
    "        print(f\"\\n{error_type.upper().replace('_', ' ')}:\")\n",
    "        print(f\"Description: {ERROR_CATEGORIES[error_type]}\")\n",
    "        print(f\"Occurrences: {len(examples)}\")\n",
    "        \n",
    "        # Show one example\n",
    "        example = examples[0]\n",
    "        print(f\"Example (Model: {example['model']}):\")\n",
    "        print(f\"  Severity: {example['severity']}\")\n",
    "        print(f\"  Notes: {example['notes']}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Failure Modes and Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify common failure patterns\n",
    "print(\"Common Failure Modes:\\n\")\n",
    "\n",
    "print(\"1. EXTRACTIVE MODELS (TextRank, LexRank):\")\n",
    "print(\"   - Tend to miss important context\")\n",
    "print(\"   - May select grammatically awkward sentence sequences\")\n",
    "print(\"   - Limited by sentence boundaries\")\n",
    "print(\"   - No rephrasing capability\")\n",
    "\n",
    "print(\"\\n2. ABSTRACTIVE MODELS (BART, Sliding Window):\")\n",
    "print(\"   - Prone to hallucinations\")\n",
    "print(\"   - May introduce factual errors\")\n",
    "print(\"   - Redundancy in longer summaries\")\n",
    "print(\"   - Information loss in chunking approaches\")\n",
    "\n",
    "print(\"\\n3. LONG DOCUMENT CHALLENGES:\")\n",
    "print(\"   - Missing information from middle sections\")\n",
    "print(\"   - Bias toward beginning/end of document\")\n",
    "print(\"   - Difficulty maintaining global coherence\")\n",
    "print(\"   - Loss of hierarchical structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RECOMMENDATIONS FOR IMPROVEMENT:\\n\")\n",
    "\n",
    "print(\"1. To reduce hallucinations:\")\n",
    "print(\"   - Add faithfulness constraints during training\")\n",
    "print(\"   - Implement post-processing fact-checking\")\n",
    "print(\"   - Use extractive-then-abstractive pipeline\")\n",
    "\n",
    "print(\"\\n2. To improve coverage:\")\n",
    "print(\"   - Use hierarchical encoding to capture global structure\")\n",
    "print(\"   - Implement section-aware summarization\")\n",
    "print(\"   - Add coverage loss during training\")\n",
    "\n",
    "print(\"\\n3. To reduce redundancy:\")\n",
    "print(\"   - Add redundancy penalty in generation\")\n",
    "print(\"   - Use diverse beam search\")\n",
    "print(\"   - Post-process to remove duplicate content\")\n",
    "\n",
    "print(\"\\n4. To improve coherence:\")\n",
    "print(\"   - Better aggregation strategies for chunks\")\n",
    "print(\"   - Discourse-aware generation\")\n",
    "print(\"   - Human-in-the-loop refinement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save error analysis results\n",
    "results_dir = Path('../results/error_analysis')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save error counts\n",
    "error_df.to_csv(results_dir / 'error_distribution.csv', index=False)\n",
    "\n",
    "# Save annotations\n",
    "with open(results_dir / 'manual_annotations.json', 'w') as f:\n",
    "    json.dump(manual_annotations, f, indent=2)\n",
    "\n",
    "print(f\"\\nError analysis saved to {results_dir}\")\n",
    "print(f\"Total errors analyzed: {len(manual_annotations)}\")\n",
    "print(f\"Unique error types: {len(ERROR_CATEGORIES)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
