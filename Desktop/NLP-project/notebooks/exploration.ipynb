{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Document Summarization - Data Exploration\n",
    "\n",
    "This notebook explores the datasets and analyzes document characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name, split='train'):\n",
    "    \"\"\"Load a processed dataset.\"\"\"\n",
    "    data_path = Path('../data/processed') / dataset_name / f'{split}.json'\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Dataset {dataset_name} not found\")\n",
    "        return None\n",
    "    \n",
    "    with open(data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Load datasets\n",
    "datasets = ['arxiv', 'pubmed', 'multi_news', 'booksum', 'billsum']\n",
    "loaded_data = {}\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    data = load_dataset(dataset_name, 'train')\n",
    "    if data:\n",
    "        loaded_data[dataset_name] = data\n",
    "        print(f\"Loaded {dataset_name}: {len(data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for each dataset\n",
    "stats = []\n",
    "\n",
    "for dataset_name, data in loaded_data.items():\n",
    "    token_counts = [sample.get('token_count', 0) for sample in data]\n",
    "    num_paragraphs = [sample.get('num_paragraphs', 0) for sample in data]\n",
    "    num_sentences = [sample.get('num_sentences', 0) for sample in data]\n",
    "    \n",
    "    stats.append({\n",
    "        'Dataset': dataset_name,\n",
    "        'Samples': len(data),\n",
    "        'Avg Tokens': np.mean(token_counts),\n",
    "        'Std Tokens': np.std(token_counts),\n",
    "        'Min Tokens': np.min(token_counts),\n",
    "        'Max Tokens': np.max(token_counts),\n",
    "        'Avg Paragraphs': np.mean(num_paragraphs),\n",
    "        'Avg Sentences': np.mean(num_sentences),\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Length Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot token count distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (dataset_name, data) in enumerate(loaded_data.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    token_counts = [sample.get('token_count', 0) for sample in data]\n",
    "    \n",
    "    axes[idx].hist(token_counts, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'{dataset_name.title()} - Token Distribution')\n",
    "    axes[idx].set_xlabel('Token Count')\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "    axes[idx].axvline(np.mean(token_counts), color='r', linestyle='--', label='Mean')\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze paragraphs per document\n",
    "for dataset_name, data in loaded_data.items():\n",
    "    num_paragraphs = [sample.get('num_paragraphs', 0) for sample in data]\n",
    "    \n",
    "    print(f\"\\n{dataset_name.upper()}:\")\n",
    "    print(f\"  Average paragraphs: {np.mean(num_paragraphs):.1f}\")\n",
    "    print(f\"  Median paragraphs: {np.median(num_paragraphs):.1f}\")\n",
    "    print(f\"  Min-Max: {np.min(num_paragraphs)}-{np.max(num_paragraphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample from each dataset\n",
    "for dataset_name, data in loaded_data.items():\n",
    "    if len(data) > 0:\n",
    "        sample = data[0]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Sample from {dataset_name.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Tokens: {sample.get('token_count', 0)}\")\n",
    "        print(f\"Paragraphs: {sample.get('num_paragraphs', 0)}\")\n",
    "        print(f\"Sentences: {sample.get('num_sentences', 0)}\")\n",
    "        \n",
    "        # Print first paragraph\n",
    "        if 'paragraphs' in sample and len(sample['paragraphs']) > 0:\n",
    "            print(f\"\\nFirst paragraph:\")\n",
    "            print(sample['paragraphs'][0][:500] + '...')\n",
    "        \n",
    "        break  # Only show one dataset for brevity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save statistics to CSV\n",
    "stats_df.to_csv('../data/processed/dataset_statistics.csv', index=False)\n",
    "print(\"Statistics saved to data/processed/dataset_statistics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
